{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/ES+d/EgA1/JqgRAL2R7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darpan02-cypher/Knowledge-Data-and-Discovery/blob/main/Word_association_minning_HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iWpF1yUGN3Gc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23101b53-b79f-4465-9f54-d48f56821b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Files found the folder:\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the path to your folder in Google Drive\n",
        "folder_path = '/content/drive/MyDrive/KDD/MovieReviews' # Replace with the actual path to your folder\n",
        "\n",
        "# List files in the folder\n",
        "try:\n",
        "    files = os.listdir(folder_path)\n",
        "    print(\"Files found the folder:\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Folder not found at {folder_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PfPPR2fAMDui"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "738e2671"
      },
      "source": [
        "# Task\n",
        "Calculate the entropy of the word \"director\" over its distribution across the 180 documents located in the folder \"/content/drive/MyDrive/document_corpus\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7d05e6b"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install `PyShortTextCategorization`, `scipy`, and `numpy`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "67814f43",
        "outputId": "bc0b299c-d5af-4621-8ca2-ba51286d8b48"
      },
      "source": [
        "!pip install -U git+https://github.com/stephenhky/PyShortTextCategorization"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/stephenhky/PyShortTextCategorization\n",
            "  Cloning https://github.com/stephenhky/PyShortTextCategorization to /tmp/pip-req-build-115vn1gn\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/stephenhky/PyShortTextCategorization /tmp/pip-req-build-115vn1gn\n",
            "  Resolved https://github.com/stephenhky/PyShortTextCategorization to commit 281f86e4b19f311d70a541179894815290f24c5a\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (1.5.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (1.6.1)\n",
            "Requirement already satisfied: tensorflow>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (2.19.0)\n",
            "Requirement already satisfied: keras>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (3.10.0)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (4.3.3)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (2.2.2)\n",
            "Requirement already satisfied: snowballstemmer>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (0.60.0)\n",
            "Requirement already satisfied: deprecation>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation>=2.0.0->shorttext==3.0.0) (25.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim>=4.0.0->shorttext==3.0.0) (7.3.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=2.13.0->shorttext==3.0.0) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=2.13.0->shorttext==3.0.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=2.13.0->shorttext==3.0.0) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=2.13.0->shorttext==3.0.0) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=2.13.0->shorttext==3.0.0) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=2.13.0->shorttext==3.0.0) (0.5.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.57.0->shorttext==3.0.0) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->shorttext==3.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->shorttext==3.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->shorttext==3.0.0) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2.0->shorttext==3.0.0) (3.6.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (1.75.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (2.19.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=2.13.0->shorttext==3.0.0) (0.45.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.13.0->shorttext==3.0.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.13.0->shorttext==3.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.13.0->shorttext==3.0.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.13.0->shorttext==3.0.0) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.13.0->shorttext==3.0.0) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.13.0->shorttext==3.0.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.13.0->shorttext==3.0.0) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=2.13.0->shorttext==3.0.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=2.13.0->shorttext==3.0.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.13.0->shorttext==3.0.0) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.13.0->shorttext==3.0.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "738fd6c9",
        "outputId": "72afaa97-5cf7-4965-b5d7-a8da843f774e"
      },
      "source": [
        "!pip install scipy numpy"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c29fadb6"
      },
      "source": [
        "## Load documents\n",
        "\n",
        "### Subtask:\n",
        "Load the documents from the specified folder in Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03cf5da8",
        "outputId": "7c143bc6-f8d9-4865-80d1-1b03ba302f34"
      },
      "source": [
        "import os\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/KDD/MovieReviews'\n",
        "documents = []\n",
        "\n",
        "try:\n",
        "    files = os.listdir(folder_path)\n",
        "    print(f\"Files found in the folder: {len(files)}\")\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        with open(file_path, 'r', encoding='latin-1') as f:\n",
        "            content = f.read()\n",
        "            documents.append(content)\n",
        "\n",
        "    print(f\"Successfully loaded {len(documents)} documents.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Folder not found at {folder_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files found in the folder: 180\n",
            "Successfully loaded 180 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2548fc4"
      },
      "source": [
        "## Count word occurrences\n",
        "\n",
        "### Subtask:\n",
        "Count the occurrences of the word \"director\" in all the documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2968020"
      },
      "source": [
        "**Reasoning**:\n",
        "Count the occurrences of the word \"director\" in each document and sum them up.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d5e7d23",
        "outputId": "d9554736-b36d-4953-c481-6c47bab96b15"
      },
      "source": [
        "director_count = 0\n",
        "for doc in documents:\n",
        "    director_count += doc.lower().count(\"director\")\n",
        "\n",
        "print(f\"Total occurrences of 'director': {director_count}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total occurrences of 'director': 292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97c5ea01"
      },
      "source": [
        "## Calculate probability distribution\n",
        "\n",
        "### Subtask:\n",
        "Calculate the probability distribution of the word \"director\" across the documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90bdf604"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate the probability distribution of the word \"director\" across the documents by dividing the total count of \"director\" by the total number of documents and calculating the complement probability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f4121b6",
        "outputId": "d4285b4c-2f9a-4b31-8b45-04e2ec69cfe4"
      },
      "source": [
        "total_documents = len(documents)\n",
        "p = director_count / total_documents\n",
        "one_minus_p = 1 - p\n",
        "\n",
        "print(f\"Total documents: {total_documents}\")\n",
        "print(f\"Probability of 'director' occurring (p): {p}\")\n",
        "print(f\"Probability of 'director' not occurring (1-p): {one_minus_p}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents: 180\n",
            "Probability of 'director' occurring (p): 1.6222222222222222\n",
            "Probability of 'director' not occurring (1-p): -0.6222222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2caf659"
      },
      "source": [
        "## Calculate probability distribution\n",
        "\n",
        "### Subtask:\n",
        "Calculate the probability distribution of the word \"director\" across the documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbdf1129",
        "outputId": "aeb06bd9-fd06-4837-f17a-6f2d5f2fef6c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "director_probabilities = []\n",
        "for doc in documents:\n",
        "    if \"director\" in doc.lower():\n",
        "        director_probabilities.append(1)\n",
        "    else:\n",
        "        director_probabilities.append(0)\n",
        "\n",
        "director_probabilities = np.array(director_probabilities)\n",
        "p = np.mean(director_probabilities)\n",
        "one_minus_p = 1 - p\n",
        "\n",
        "print(f\"Probability of 'director' occurring (p): {p}\")\n",
        "print(f\"Probability of 'director' not occurring (1-p): {one_minus_p}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of 'director' occurring (p): 0.9388888888888889\n",
            "Probability of 'director' not occurring (1-p): 0.061111111111111116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "368fb289"
      },
      "source": [
        "## Calculate entropy\n",
        "\n",
        "### Subtask:\n",
        "Compute the entropy of the word \"director\" using the calculated probability distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45a0a92a",
        "outputId": "e3e30c1f-e909-4da7-bd97-817541c09259"
      },
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "probabilities = [p, one_minus_p]\n",
        "entropy_value = entropy(probabilities, base=2)\n",
        "\n",
        "entropy = entropy_value\n",
        "\n",
        "print(f\"Entropy of 'director': {entropy}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of 'director': 0.3318399155702335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87a4b46b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   180 documents were successfully loaded from the path `/content/drive/MyDrive/KDD/MovieReviews` using 'latin-1' encoding.\n",
        "*   The word \"director\" (case-insensitive) appeared a total of 292 times across all documents.\n",
        "*   The probability of the word \"director\" occurring in a document is approximately 0.939.\n",
        "*   The probability of the word \"director\" not occurring in a document is approximately 0.061.\n",
        "*   The calculated entropy of the word \"director\" over its distribution across the documents is approximately 0.3318.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The initial attempt to calculate the probability distribution based on total occurrences led to an incorrect probability greater than 1, highlighting the importance of correctly defining the event for probability calculation (occurrence in a document vs. total count).\n",
        "*   The low entropy value suggests that the word \"director\" is relatively predictable in this corpus, frequently appearing in documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Correct result of mutual information between the “director” and the document author."
      ],
      "metadata": {
        "id": "tjLOda3Rgikr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19dc44da"
      },
      "source": [
        "To calculate the mutual information between the word \"director\" and the document author, we need the following:\n",
        "\n",
        "1.  A list of documents.\n",
        "2.  A list of authors, where each author in the list corresponds to the author of the document at the same index in the document list.\n",
        "\n",
        "Assuming we have these two lists, we can use the following code to calculate the mutual information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecb7f28f",
        "outputId": "12f1fa77-e6d6-4c6f-f6bc-1e5320dec05b"
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Based on the  information, the first 80 reviews are by Berardinelli and the next 100 by Schwartz.\n",
        "document_authors = [\"Berardinelli\"] * 80 + [\"Schwartz\"] * 100\n",
        "\n",
        "# Ensure the number of documents and authors are the same\n",
        "if len(documents) != len(document_authors):\n",
        "    raise ValueError(\"The number of documents and authors must be the same.\")\n",
        "\n",
        "# Create a joint probability distribution of (word_occurrence, author)\n",
        "joint_prob_dist = defaultdict(int)\n",
        "word_occurrences = [1 if \"director\" in doc.lower() else 0 for doc in documents]\n",
        "\n",
        "for word_occurrence, author in zip(word_occurrences, document_authors):\n",
        "    joint_prob_dist[(word_occurrence, author)] += 1\n",
        "\n",
        "# Normalize the joint probability distribution\n",
        "total_pairs = len(documents)\n",
        "for key in joint_prob_dist:\n",
        "    joint_prob_dist[key] /= total_pairs\n",
        "\n",
        "# Calculate marginal probability distribution of word occurrence\n",
        "p_word = {0: 0, 1: 0}\n",
        "for word_occurrence in word_occurrences:\n",
        "    p_word[word_occurrence] += 1\n",
        "for key in p_word:\n",
        "    p_word[key] /= total_pairs\n",
        "\n",
        "# Calculate marginal probability distribution of author\n",
        "p_author = defaultdict(int)\n",
        "for author in document_authors:\n",
        "    p_author[author] += 1\n",
        "for key in p_author:\n",
        "    p_author[key] /= total_pairs\n",
        "\n",
        "# Calculate mutual information\n",
        "mutual_information = 0\n",
        "for (word_occurrence, author), p_xy in joint_prob_dist.items():\n",
        "    if p_xy > 0:\n",
        "        p_x = p_word[word_occurrence]\n",
        "        p_y = p_author[author]\n",
        "        if p_x > 0 and p_y > 0:\n",
        "            mutual_information += p_xy * np.log2(p_xy / (p_x * p_y))\n",
        "\n",
        "print(f\"Mutual Information between 'director' and document author: {mutual_information}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutual Information between 'director' and document author: 0.0751048474130945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Find the top ten words with the highest mutual information with the document author andtheir respective mutual information. Explain (in Python comments) that what it means for word by having a high mutual information with the document author.\n"
      ],
      "metadata": {
        "id": "f5pjXdT62lRH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d081570e"
      },
      "source": [
        "# Task\n",
        "Find the top ten words with the highest mutual information with the document author and their respective mutual information using the corpus of 180 movie reviews, where the first 80 reviews are by Berardinelli and the remaining 100 are by Schwartz. Explain what it means for a word to have a high mutual information with the document author."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2b97dc6"
      },
      "source": [
        "## Tokenize documents\n",
        "\n",
        "### Subtask:\n",
        "Split the documents into individual words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "540ef4fc"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires tokenizing the documents into individual words. I will use `nltk` for this purpose, which requires downloading the 'punkt' tokenizer data first. Then I will iterate through the documents, convert them to lowercase, tokenize them, and store the tokens in a list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "920e78a2",
        "outputId": "d4db8dc1-e540-4f4f-b1e2-7e68149f3efa"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt' tokenizer data if not already present\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "tokenized_documents = []\n",
        "for doc in documents:\n",
        "    # Convert to lowercase and tokenize\n",
        "    tokens = word_tokenize(doc.lower())\n",
        "    tokenized_documents.append(tokens)\n",
        "\n",
        "print(f\"Successfully tokenized {len(tokenized_documents)} documents.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully tokenized 180 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec9583b0"
      },
      "source": [
        "## Calculate mutual information for each word\n",
        "\n",
        "### Subtask:\n",
        "For each unique word, calculate its mutual information with the document author.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e86514df"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate the mutual information for each unique word across all documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "748b031f",
        "outputId": "75b4c514-7340-4460-b2be-dbe805f37917"
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# Create a set of all unique words across all tokenized_documents.\n",
        "all_words = set()\n",
        "for doc_tokens in tokenized_documents:\n",
        "    all_words.update(doc_tokens)\n",
        "\n",
        "# Initialize an empty dictionary to store the mutual information for each word.\n",
        "word_mutual_information = {}\n",
        "\n",
        "# Calculate marginal probability distribution of author (reused from previous calculation)\n",
        "p_author = defaultdict(int)\n",
        "for author in document_authors:\n",
        "    p_author[author] += 1\n",
        "total_pairs = len(documents)\n",
        "for key in p_author:\n",
        "    p_author[key] /= total_pairs\n",
        "\n",
        "# Iterate through each unique word.\n",
        "for word in all_words:\n",
        "    # For each word, create a list indicating its occurrence (1 if present, 0 if not) in each document.\n",
        "    word_occurrences = [1 if word in doc_tokens else 0 for doc_tokens in tokenized_documents]\n",
        "\n",
        "    # Calculate the joint probability distribution of the word occurrence and the document author.\n",
        "    joint_prob_dist = defaultdict(int)\n",
        "    for word_occurrence, author in zip(word_occurrences, document_authors):\n",
        "        joint_prob_dist[(word_occurrence, author)] += 1\n",
        "\n",
        "    # Normalize the joint probability distribution\n",
        "    for key in joint_prob_dist:\n",
        "        joint_prob_dist[key] /= total_pairs\n",
        "\n",
        "    # Calculate marginal probability distribution of word occurrence\n",
        "    p_word = {0: 0, 1: 0}\n",
        "    for word_occurrence in word_occurrences:\n",
        "        p_word[word_occurrence] += 1\n",
        "    for key in p_word:\n",
        "        p_word[key] /= total_pairs\n",
        "\n",
        "    # Compute the mutual information for the current word\n",
        "    mutual_information = 0\n",
        "    for (word_occurrence, author), p_xy in joint_prob_dist.items():\n",
        "        if p_xy > 0:\n",
        "            p_x = p_word[word_occurrence]\n",
        "            p_y = p_author[author]\n",
        "            if p_x > 0 and p_y > 0:\n",
        "                mutual_information += p_xy * np.log2(p_xy / (p_x * p_y))\n",
        "\n",
        "    # Store the mutual information in the dictionary.\n",
        "    word_mutual_information[word] = mutual_information\n",
        "\n",
        "print(f\"Calculated mutual information for {len(word_mutual_information)} unique words.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated mutual information for 15760 unique words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2472f658"
      },
      "source": [
        "## Rank words by mutual information\n",
        "\n",
        "### Subtask:\n",
        "Sort the words based on their calculated mutual information in descending order.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c691707"
      },
      "source": [
        "**Reasoning**:\n",
        "Sort the word_mutual_information dictionary items by mutual information in descending order and store them in a new list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dec7229",
        "outputId": "563f24ed-3d4a-43ca-99f3-e477470c2555"
      },
      "source": [
        "# Sort the word_mutual_information dictionary by values (mutual information) in descending order\n",
        "sorted_word_mutual_information = sorted(word_mutual_information.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "print(f\"Successfully sorted words by mutual information.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully sorted words by mutual information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1027eff9"
      },
      "source": [
        "## Select top ten words\n",
        "\n",
        "### Subtask:\n",
        "Get the top ten words with the highest mutual information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75a73f54"
      },
      "source": [
        "**Reasoning**:\n",
        "Select the first 10 elements from the sorted list and store them in a new variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13e8b57b",
        "outputId": "5f959172-616b-4d1c-b47a-483b2f3f5dc0"
      },
      "source": [
        "top_ten_words = sorted_word_mutual_information[:10]\n",
        "\n",
        "print(\"Top ten words with the highest mutual information with document author:\")\n",
        "for word, mi in top_ten_words:\n",
        "    print(f\"Word: '{word}', Mutual Information: {mi}\")\n",
        "\n",
        "# Explain the meaning of high mutual information\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"A word having high mutual information with the document author indicates that the word's presence or absence in a document is strongly associated with a particular author.\")\n",
        "print(\"In other words, the word is more likely to appear in documents written by one author compared to the other.\")\n",
        "print(\"This suggests that the word is a distinguishing feature of that author's writing style or the topics they tend to cover.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top ten words with the highest mutual information with document author:\n",
            "Word: 'schwartz', Mutual Information: 0.991076059838222\n",
            "Word: 'rights', Mutual Information: 0.9031044927571723\n",
            "Word: 'reserved', Mutual Information: 0.869351835367191\n",
            "Word: 'dennis', Mutual Information: 0.8062750920498906\n",
            "Word: 'reviewed', Mutual Information: 0.7221798550488772\n",
            "Word: '©', Mutual Information: 0.6302403387276296\n",
            "Word: 'reviews', Mutual Information: 0.5237955538568037\n",
            "Word: 'cast', Mutual Information: 0.49374267516494463\n",
            "Word: ';', Mutual Information: 0.47696373366128547\n",
            "Word: '--', Mutual Information: 0.4713735184568902\n",
            "\n",
            "Explanation:\n",
            "A word having high mutual information with the document author indicates that the word's presence or absence in a document is strongly associated with a particular author.\n",
            "In other words, the word is more likely to appear in documents written by one author compared to the other.\n",
            "This suggests that the word is a distinguishing feature of that author's writing style or the topics they tend to cover.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4360a2c2"
      },
      "source": [
        "## Display results and explanation\n",
        "\n",
        "### Subtask:\n",
        "Print the top ten words and their mutual information values, and explain what high mutual information means in this context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f9864cd"
      },
      "source": [
        "**Reasoning**:\n",
        "Print the top ten words and their mutual information values, and explain what high mutual information means in this context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2c39ce0",
        "outputId": "2d50acda-4366-4918-d03a-832323214c82"
      },
      "source": [
        "# Print the top ten words and their mutual information\n",
        "print(\"Top ten words with the highest mutual information with document author:\")\n",
        "for word, mi in top_ten_words:\n",
        "    print(f\"Word: '{word}', Mutual Information: {mi:.4f}\")\n",
        "\n",
        "# Explain the meaning of high mutual information\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"A word having high mutual information with the document author indicates that the word's presence or absence in a document is strongly associated with a particular author.\")\n",
        "print(\"In other words, the word is more likely to appear in documents written by one author compared to the other.\")\n",
        "print(\"This suggests that the word is a distinguishing feature of that author's writing style or the topics they tend to cover.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top ten words with the highest mutual information with document author:\n",
            "Word: 'schwartz', Mutual Information: 0.9911\n",
            "Word: 'rights', Mutual Information: 0.9031\n",
            "Word: 'reserved', Mutual Information: 0.8694\n",
            "Word: 'dennis', Mutual Information: 0.8063\n",
            "Word: 'reviewed', Mutual Information: 0.7222\n",
            "Word: '©', Mutual Information: 0.6302\n",
            "Word: 'reviews', Mutual Information: 0.5238\n",
            "Word: 'cast', Mutual Information: 0.4937\n",
            "Word: ';', Mutual Information: 0.4770\n",
            "Word: '--', Mutual Information: 0.4714\n",
            "\n",
            "Explanation:\n",
            "A word having high mutual information with the document author indicates that the word's presence or absence in a document is strongly associated with a particular author.\n",
            "In other words, the word is more likely to appear in documents written by one author compared to the other.\n",
            "This suggests that the word is a distinguishing feature of that author's writing style or the topics they tend to cover.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1044ee9"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "What does it mean for a word to have a high mutual information with the document author?\n",
        "A word having high mutual information with the document author indicates that the word's presence or absence in a document is strongly associated with a particular author. This suggests that the word is a distinguishing feature of that author's writing style or the topics they tend to cover, as the word is more likely to appear in documents written by one author compared to the other.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "* The top ten words with the highest mutual information with the document author are: 'schwartz' (0.9911), 'rights' (0.9031), 'reserved' (0.8694), 'dennis' (0.8063), 'reviewed' (0.7222), '©' (0.6302), 'reviews' (0.5238), 'cast' (0.4937), ';' (0.4770), and '--' (0.4714).\n",
        "* The word 'schwartz' has the highest mutual information, indicating a strong association with the author Schwartz.\n",
        "* Words like 'rights', 'reserved', and 'dennis' also show high mutual information, suggesting they are highly indicative of a specific author (likely Schwartz and Berardinelli, respectively, based on the names).\n",
        "\n"
      ]
    }
  ]
}