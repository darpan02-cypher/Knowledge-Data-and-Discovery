{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darpan02-cypher/Knowledge-Data-and-Discovery/blob/main/WordAssociationMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkHF5qsY3zNM",
        "outputId": "6034e999-6913-427c-e5fe-3000c0e5e5b0"
      },
      "source": [
        "!pip install -U git+https://github.com/stephenhky/PyShortTextCategorization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/stephenhky/PyShortTextCategorization\n",
            "  Cloning https://github.com/stephenhky/PyShortTextCategorization to /tmp/pip-req-build-y0mmssw5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/stephenhky/PyShortTextCategorization /tmp/pip-req-build-y0mmssw5\n",
            "  Resolved https://github.com/stephenhky/PyShortTextCategorization to commit ef2de02cc5d8115126efa99672b8cf69e506b47d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (1.5.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (1.7.2)\n",
            "Requirement already satisfied: tensorflow>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (2.20.0)\n",
            "Requirement already satisfied: keras>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (3.11.3)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (4.3.3)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (2.3.2)\n",
            "Requirement already satisfied: snowballstemmer>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (0.62.0)\n",
            "Requirement already satisfied: deprecation>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from shorttext==3.0.0) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation>=2.0.0->shorttext==3.0.0) (25.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim>=4.0.0->shorttext==3.0.0) (7.3.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=2.13.0->shorttext==3.0.0) (2.3.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=2.13.0->shorttext==3.0.0) (14.1.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=2.13.0->shorttext==3.0.0) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=2.13.0->shorttext==3.0.0) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=2.13.0->shorttext==3.0.0) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=2.13.0->shorttext==3.0.0) (0.5.3)\n",
            "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.57.0->shorttext==3.0.0) (0.45.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->shorttext==3.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->shorttext==3.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->shorttext==3.0.0) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2.0->shorttext==3.0.0) (3.6.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (0.6.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (6.32.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (2.32.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (80.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (3.1.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (1.75.0)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.13.0->shorttext==3.0.0) (2.20.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=2.13.0->shorttext==3.0.0) (0.45.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.13.0->shorttext==3.0.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.13.0->shorttext==3.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.13.0->shorttext==3.0.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.13.0->shorttext==3.0.0) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow>=2.13.0->shorttext==3.0.0) (3.9)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow>=2.13.0->shorttext==3.0.0) (11.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow>=2.13.0->shorttext==3.0.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow>=2.13.0->shorttext==3.0.0) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=2.13.0->shorttext==3.0.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=2.13.0->shorttext==3.0.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.13.0->shorttext==3.0.0) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow>=2.13.0->shorttext==3.0.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0fAOrGAM70W",
        "outputId": "b9c70b3a-33be-499a-9bfd-a16eca6ffc84"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scipy==1.12.* numpy==1.25.*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imx38oqlwNCM",
        "outputId": "2e388f3f-a645-46d1-ed75-0aa9b3f65b2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.12.*\n",
            "  Using cached scipy-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting numpy==1.25.*\n",
            "  Using cached numpy-1.25.2.tar.gz (10.8 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvhlPcdW4M-w",
        "outputId": "d47b70c0-a2a3-4376-a48a-4c5248cf6d26"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD-z1ENv4SZG",
        "outputId": "6c9bec09-e37f-4fd3-a2e5-7de86009b49d"
      },
      "source": [
        "# %cd /content/drive/My Drive/ColabData\n",
        "%cd /content/drive/MyDrive/KDD\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/KDD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import PlaintextCorpusReader\n",
        "corpus_root = 'HealthNews' # Folder Name\n",
        "filelists = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')  # wildcard is read all files in the folder"
      ],
      "metadata": {
        "id": "_OW_pkWTN3fL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a74c4676",
        "outputId": "384c6475-c46b-4302-95b3-00aa2c36c85b"
      },
      "source": [
        "!pip install -U shorttext nltk --force-reinstall"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shorttext\n",
            "  Using cached shorttext-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nltk\n",
            "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting numpy>=1.23.3 (from shorttext)\n",
            "  Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting scipy>=1.12.0 (from shorttext)\n",
            "  Using cached scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting joblib>=1.3.0 (from shorttext)\n",
            "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting scikit-learn>=1.2.0 (from shorttext)\n",
            "  Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting tensorflow>=2.13.0 (from shorttext)\n",
            "  Using cached tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting keras>=2.13.0 (from shorttext)\n",
            "  Using cached keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting gensim>=4.0.0 (from shorttext)\n",
            "  Using cached gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting pandas>=1.2.0 (from shorttext)\n",
            "  Using cached pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "Collecting snowballstemmer>=3.0.0 (from shorttext)\n",
            "  Using cached snowballstemmer-3.0.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting numba>=0.57.0 (from shorttext)\n",
            "  Using cached numba-0.62.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting deprecation>=2.0.0 (from shorttext)\n",
            "  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting click (from nltk)\n",
            "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Using cached regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting tqdm (from nltk)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting packaging (from deprecation>=2.0.0->shorttext)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting numpy>=1.23.3 (from shorttext)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy>=1.12.0 (from shorttext)\n",
            "  Using cached scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim>=4.0.0->shorttext)\n",
            "  Using cached smart_open-7.3.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting absl-py (from keras>=2.13.0->shorttext)\n",
            "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting rich (from keras>=2.13.0->shorttext)\n",
            "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting namex (from keras>=2.13.0->shorttext)\n",
            "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
            "Collecting h5py (from keras>=2.13.0->shorttext)\n",
            "  Using cached h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting optree (from keras>=2.13.0->shorttext)\n",
            "  Using cached optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
            "Collecting ml-dtypes (from keras>=2.13.0->shorttext)\n",
            "  Using cached ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.57.0->shorttext)\n",
            "  Using cached llvmlite-0.45.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas>=1.2.0->shorttext)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas>=1.2.0->shorttext)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas>=1.2.0->shorttext)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.2.0->shorttext)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting opt_einsum>=2.3.2 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting protobuf>=5.28.0 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting requests<3,>=2.21.0 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting setuptools (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting six>=1.12.0 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typing_extensions>=3.6.6 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached grpcio-1.75.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow>=2.13.0->shorttext)\n",
            "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow>=2.13.0->shorttext)\n",
            "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow>=2.13.0->shorttext)\n",
            "  Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow>=2.13.0->shorttext)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow>=2.13.0->shorttext)\n",
            "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow>=2.13.0->shorttext)\n",
            "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow>=2.13.0->shorttext)\n",
            "  Using cached markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pillow (from tensorboard~=2.20.0->tensorflow>=2.13.0->shorttext)\n",
            "  Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow>=2.13.0->shorttext)\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow>=2.13.0->shorttext)\n",
            "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->keras>=2.13.0->shorttext)\n",
            "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich->keras>=2.13.0->shorttext)\n",
            "  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=2.13.0->shorttext)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow>=2.13.0->shorttext)\n",
            "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Using cached shorttext-3.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Using cached keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
            "Using cached numba-0.62.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Using cached pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "Using cached regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)\n",
            "Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
            "Using cached scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "Using cached snowballstemmer-3.0.1-py3-none-any.whl (103 kB)\n",
            "Using cached tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Using cached grpcio-1.75.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.4 MB)\n",
            "Using cached h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "Using cached llvmlite-0.45.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "Using cached ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
            "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Using cached protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached smart_open-7.3.1-py3-none-any.whl (61 kB)\n",
            "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Using cached wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
            "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
            "Using cached optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
            "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
            "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached markdown-3.9-py3-none-any.whl (107 kB)\n",
            "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
            "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: pytz, namex, libclang, flatbuffers, wrapt, wheel, urllib3, tzdata, typing_extensions, tqdm, threadpoolctl, termcolor, tensorboard-data-server, snowballstemmer, six, setuptools, regex, pygments, protobuf, pillow, packaging, opt_einsum, numpy, mdurl, MarkupSafe, markdown, llvmlite, joblib, idna, gast, click, charset_normalizer, certifi, absl-py, werkzeug, smart-open, scipy, requests, python-dateutil, optree, numba, nltk, ml-dtypes, markdown-it-py, h5py, grpcio, google_pasta, deprecation, astunparse, tensorboard, scikit-learn, rich, pandas, gensim, keras, tensorflow, shorttext\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: namex\n",
            "    Found existing installation: namex 0.1.0\n",
            "    Uninstalling namex-0.1.0:\n",
            "      Successfully uninstalled namex-0.1.0\n",
            "  Attempting uninstall: libclang\n",
            "    Found existing installation: libclang 18.1.1\n",
            "    Uninstalling libclang-18.1.1:\n",
            "      Successfully uninstalled libclang-18.1.1\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 25.9.23\n",
            "    Uninstalling flatbuffers-25.9.23:\n",
            "      Successfully uninstalled flatbuffers-25.9.23\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.3\n",
            "    Uninstalling wrapt-1.17.3:\n",
            "      Successfully uninstalled wrapt-1.17.3\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.45.1\n",
            "    Uninstalling wheel-0.45.1:\n",
            "      Successfully uninstalled wheel-0.45.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 3.1.0\n",
            "    Uninstalling termcolor-3.1.0:\n",
            "      Successfully uninstalled termcolor-3.1.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: snowballstemmer\n",
            "    Found existing installation: snowballstemmer 3.0.1\n",
            "    Uninstalling snowballstemmer-3.0.1:\n",
            "      Successfully uninstalled snowballstemmer-3.0.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 80.9.0\n",
            "    Uninstalling setuptools-80.9.0:\n",
            "      Successfully uninstalled setuptools-80.9.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2025.9.18\n",
            "    Uninstalling regex-2025.9.18:\n",
            "      Successfully uninstalled regex-2025.9.18\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.19.2\n",
            "    Uninstalling Pygments-2.19.2:\n",
            "      Successfully uninstalled Pygments-2.19.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 6.32.1\n",
            "    Uninstalling protobuf-6.32.1:\n",
            "      Successfully uninstalled protobuf-6.32.1\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: opt_einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: mdurl\n",
            "    Found existing installation: mdurl 0.1.2\n",
            "    Uninstalling mdurl-0.1.2:\n",
            "      Successfully uninstalled mdurl-0.1.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.9\n",
            "    Uninstalling Markdown-3.9:\n",
            "      Successfully uninstalled Markdown-3.9\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.45.0\n",
            "    Uninstalling llvmlite-0.45.0:\n",
            "      Successfully uninstalled llvmlite-0.45.0\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.2\n",
            "    Uninstalling joblib-1.5.2:\n",
            "      Successfully uninstalled joblib-1.5.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "  Attempting uninstall: charset_normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.3\n",
            "    Uninstalling charset-normalizer-3.4.3:\n",
            "      Successfully uninstalled charset-normalizer-3.4.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.8.3\n",
            "    Uninstalling certifi-2025.8.3:\n",
            "      Successfully uninstalled certifi-2025.8.3\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 2.3.1\n",
            "    Uninstalling absl-py-2.3.1:\n",
            "      Successfully uninstalled absl-py-2.3.1\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart_open 7.3.1\n",
            "    Uninstalling smart_open-7.3.1:\n",
            "      Successfully uninstalled smart_open-7.3.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.5\n",
            "    Uninstalling requests-2.32.5:\n",
            "      Successfully uninstalled requests-2.32.5\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: optree\n",
            "    Found existing installation: optree 0.17.0\n",
            "    Uninstalling optree-0.17.0:\n",
            "      Successfully uninstalled optree-0.17.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.62.0\n",
            "    Uninstalling numba-0.62.0:\n",
            "      Successfully uninstalled numba-0.62.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.3\n",
            "    Uninstalling ml_dtypes-0.5.3:\n",
            "      Successfully uninstalled ml_dtypes-0.5.3\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 4.0.0\n",
            "    Uninstalling markdown-it-py-4.0.0:\n",
            "      Successfully uninstalled markdown-it-py-4.0.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.14.0\n",
            "    Uninstalling h5py-3.14.0:\n",
            "      Successfully uninstalled h5py-3.14.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.75.0\n",
            "    Uninstalling grpcio-1.75.0:\n",
            "      Successfully uninstalled grpcio-1.75.0\n",
            "  Attempting uninstall: google_pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: deprecation\n",
            "    Found existing installation: deprecation 2.1.0\n",
            "    Uninstalling deprecation-2.1.0:\n",
            "      Successfully uninstalled deprecation-2.1.0\n",
            "  Attempting uninstall: astunparse\n",
            "    Found existing installation: astunparse 1.6.3\n",
            "    Uninstalling astunparse-1.6.3:\n",
            "      Successfully uninstalled astunparse-1.6.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.20.0\n",
            "    Uninstalling tensorboard-2.20.0:\n",
            "      Successfully uninstalled tensorboard-2.20.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.7.2\n",
            "    Uninstalling scikit-learn-1.7.2:\n",
            "      Successfully uninstalled scikit-learn-1.7.2\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 14.1.0\n",
            "    Uninstalling rich-14.1.0:\n",
            "      Successfully uninstalled rich-14.1.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.3.2\n",
            "    Uninstalling pandas-2.3.2:\n",
            "      Successfully uninstalled pandas-2.3.2\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.11.3\n",
            "    Uninstalling keras-3.11.3:\n",
            "      Successfully uninstalled keras-3.11.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.20.0\n",
            "    Uninstalling tensorflow-2.20.0:\n",
            "      Successfully uninstalled tensorflow-2.20.0\n",
            "  Attempting uninstall: shorttext\n",
            "    Found existing installation: shorttext 3.0.0\n",
            "    Uninstalling shorttext-3.0.0:\n",
            "      Successfully uninstalled shorttext-3.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n",
            "cuml-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, but you have numba 0.62.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.32.1 which is incompatible.\n",
            "bigframes 2.21.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.1 which is incompatible.\n",
            "dask-cuda 25.6.0 requires numba<0.62.0a0,>=0.59.1, but you have numba 0.62.0 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.20.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, but you have numba 0.62.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "distributed-ucxx-cu12 0.44.0 requires numba<0.62.0a0,>=0.59.1, but you have numba 0.62.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 absl-py-2.3.1 astunparse-1.6.3 certifi-2025.8.3 charset_normalizer-3.4.3 click-8.3.0 deprecation-2.1.0 flatbuffers-25.9.23 gast-0.6.0 gensim-4.3.3 google_pasta-0.2.0 grpcio-1.75.0 h5py-3.14.0 idna-3.10 joblib-1.5.2 keras-3.11.3 libclang-18.1.1 llvmlite-0.45.0 markdown-3.9 markdown-it-py-4.0.0 mdurl-0.1.2 ml-dtypes-0.5.3 namex-0.1.0 nltk-3.9.1 numba-0.62.0 numpy-1.26.4 opt_einsum-3.4.0 optree-0.17.0 packaging-25.0 pandas-2.3.2 pillow-11.3.0 protobuf-6.32.1 pygments-2.19.2 python-dateutil-2.9.0.post0 pytz-2025.2 regex-2025.9.18 requests-2.32.5 rich-14.1.0 scikit-learn-1.7.2 scipy-1.13.1 setuptools-80.9.0 shorttext-3.0.0 six-1.17.0 smart-open-7.3.1 snowballstemmer-3.0.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 threadpoolctl-3.6.0 tqdm-4.67.1 typing_extensions-4.15.0 tzdata-2025.2 urllib3-2.5.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "_distutils_hack",
                  "certifi",
                  "dateutil",
                  "google",
                  "joblib",
                  "nltk",
                  "numpy",
                  "packaging",
                  "pandas",
                  "pytz",
                  "regex",
                  "scipy",
                  "six",
                  "sklearn",
                  "threadpoolctl"
                ]
              },
              "id": "8e91d617b537426e834bfa5a43b5edc8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7ecdd20",
        "outputId": "40857b53-26ff-4c74-af0a-15ac2099b792"
      },
      "source": [
        "!pip install -U shorttext==3.0.0 scipy==1.12.0 numpy==1.25.2 --force-reinstall"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shorttext==3.0.0\n",
            "  Using cached shorttext-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting scipy==1.12.0\n",
            "  Using cached scipy-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting numpy==1.25.2\n",
            "  Using cached numpy-1.25.2.tar.gz (10.8 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saiHTVjS11nm",
        "outputId": "d4cd0f0c-6772-4806-b732-124becd24b09"
      },
      "source": [
        "from nltk.corpus import PlaintextCorpusReader\n",
        "corpus_root = 'HealthNews' # Folder Name\n",
        "filelists = PlaintextCorpusReader(corpus_root, '.*',encoding='latin-1')  # wildcard is read all files in the folder\n",
        "filelists.fileids()  # Get the filenames"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.DS_Store',\n",
              " 'Dentistry00500.txt',\n",
              " 'Dentistry00501.txt',\n",
              " 'Dentistry00502.txt',\n",
              " 'Dentistry00503.txt',\n",
              " 'Dentistry00504.txt',\n",
              " 'Dentistry00505.txt',\n",
              " 'Dentistry00506.txt',\n",
              " 'Dentistry00507.txt',\n",
              " 'Dentistry00508.txt',\n",
              " 'Dentistry00509.txt',\n",
              " 'Dentistry00510.txt',\n",
              " 'Dentistry00511.txt',\n",
              " 'Dentistry00512.txt',\n",
              " 'Dentistry00513.txt',\n",
              " 'Dentistry00514.txt',\n",
              " 'Dentistry00515.txt',\n",
              " 'Dentistry00516.txt',\n",
              " 'Dentistry00517.txt',\n",
              " 'Dentistry00518.txt',\n",
              " 'Dentistry00519.txt',\n",
              " 'Dentistry00520.txt',\n",
              " 'Dentistry00521.txt',\n",
              " 'Dentistry00522.txt',\n",
              " 'Dentistry00523.txt',\n",
              " 'Dentistry00524.txt',\n",
              " 'Dentistry00525.txt',\n",
              " 'Dentistry00526.txt',\n",
              " 'Dentistry00527.txt',\n",
              " 'Dentistry00528.txt',\n",
              " 'Dentistry00529.txt',\n",
              " 'Dentistry00530.txt',\n",
              " 'Dentistry00531.txt',\n",
              " 'Dentistry00532.txt',\n",
              " 'Dentistry00533.txt',\n",
              " 'Dentistry00534.txt',\n",
              " 'Dentistry00535.txt',\n",
              " 'Dentistry00536.txt',\n",
              " 'Dentistry00537.txt',\n",
              " 'Dentistry00538.txt',\n",
              " 'Dentistry00539.txt',\n",
              " 'Dentistry00540.txt',\n",
              " 'Dentistry00541.txt',\n",
              " 'Dentistry00542.txt',\n",
              " 'Dentistry00543.txt',\n",
              " 'Dentistry00544.txt',\n",
              " 'Dentistry00545.txt',\n",
              " 'Dentistry00546.txt',\n",
              " 'Dentistry00547.txt',\n",
              " 'Dentistry00548.txt',\n",
              " 'Dentistry00549.txt',\n",
              " 'Dentistry00550.txt',\n",
              " 'Dentistry00551.txt',\n",
              " 'Dentistry00552.txt',\n",
              " 'Dentistry00553.txt',\n",
              " 'Dentistry00554.txt',\n",
              " 'Dentistry00555.txt',\n",
              " 'Dentistry00556.txt',\n",
              " 'Dentistry00557.txt',\n",
              " 'Dentistry00558.txt',\n",
              " 'Dentistry00559.txt',\n",
              " 'Depression00500.txt',\n",
              " 'Depression00501.txt',\n",
              " 'Depression00502.txt',\n",
              " 'Depression00503.txt',\n",
              " 'Depression00504.txt',\n",
              " 'Depression00505.txt',\n",
              " 'Depression00506.txt',\n",
              " 'Depression00507.txt',\n",
              " 'Depression00508.txt',\n",
              " 'Depression00509.txt',\n",
              " 'Depression00510.txt',\n",
              " 'Depression00511.txt',\n",
              " 'Depression00512.txt',\n",
              " 'Depression00513.txt',\n",
              " 'Depression00514.txt',\n",
              " 'Depression00515.txt',\n",
              " 'Depression00516.txt',\n",
              " 'Depression00517.txt',\n",
              " 'Depression00518.txt',\n",
              " 'Depression00519.txt',\n",
              " 'Depression00520.txt',\n",
              " 'Depression00521.txt',\n",
              " 'Depression00522.txt',\n",
              " 'Depression00523.txt',\n",
              " 'Depression00524.txt',\n",
              " 'Depression00525.txt',\n",
              " 'Depression00526.txt',\n",
              " 'Depression00527.txt',\n",
              " 'Depression00528.txt',\n",
              " 'Depression00529.txt',\n",
              " 'Depression00530.txt',\n",
              " 'Depression00531.txt',\n",
              " 'Depression00532.txt',\n",
              " 'Depression00533.txt',\n",
              " 'Depression00534.txt',\n",
              " 'Depression00535.txt',\n",
              " 'Depression00536.txt',\n",
              " 'Depression00537.txt',\n",
              " 'Depression00538.txt',\n",
              " 'Depression00539.txt',\n",
              " 'Diabetes00500.txt',\n",
              " 'Diabetes00501.txt',\n",
              " 'Diabetes00502.txt',\n",
              " 'Diabetes00503.txt',\n",
              " 'Diabetes00504.txt',\n",
              " 'Diabetes00505.txt',\n",
              " 'Diabetes00506.txt',\n",
              " 'Diabetes00507.txt',\n",
              " 'Diabetes00508.txt',\n",
              " 'Diabetes00509.txt',\n",
              " 'Diabetes00510.txt',\n",
              " 'Diabetes00511.txt',\n",
              " 'Diabetes00512.txt',\n",
              " 'Diabetes00513.txt',\n",
              " 'Diabetes00514.txt',\n",
              " 'Diabetes00515.txt',\n",
              " 'Diabetes00516.txt',\n",
              " 'Diabetes00517.txt',\n",
              " 'Diabetes00518.txt',\n",
              " 'Diabetes00519.txt',\n",
              " 'Diabetes00520.txt',\n",
              " 'Diabetes00521.txt',\n",
              " 'Diabetes00522.txt',\n",
              " 'Diabetes00523.txt',\n",
              " 'Diabetes00524.txt',\n",
              " 'Diabetes00525.txt',\n",
              " 'Diabetes00526.txt',\n",
              " 'Diabetes00527.txt',\n",
              " 'Diabetes00528.txt',\n",
              " 'Diabetes00529.txt',\n",
              " 'Diabetes00530.txt',\n",
              " 'Diabetes00531.txt',\n",
              " 'Diabetes00532.txt',\n",
              " 'Diabetes00533.txt',\n",
              " 'Diabetes00534.txt',\n",
              " 'Diabetes00535.txt',\n",
              " 'Diabetes00536.txt',\n",
              " 'Diabetes00537.txt',\n",
              " 'Diabetes00538.txt',\n",
              " 'Diabetes00539.txt',\n",
              " 'Diabetes00540.txt',\n",
              " 'Diabetes00541.txt',\n",
              " 'Diabetes00542.txt',\n",
              " 'Diabetes00543.txt',\n",
              " 'Diabetes00544.txt',\n",
              " 'Diabetes00545.txt',\n",
              " 'Diabetes00546.txt',\n",
              " 'Diabetes00547.txt',\n",
              " 'Diabetes00548.txt',\n",
              " 'Diabetes00549.txt',\n",
              " 'Diabetes00550.txt',\n",
              " 'Diabetes00551.txt',\n",
              " 'Diabetes00552.txt',\n",
              " 'Diabetes00553.txt',\n",
              " 'Diabetes00554.txt',\n",
              " 'Diabetes00555.txt',\n",
              " 'Diabetes00556.txt',\n",
              " 'Diabetes00557.txt',\n",
              " 'Diabetes00558.txt',\n",
              " 'Diabetes00559.txt',\n",
              " 'Diabetes00560.txt',\n",
              " 'Diabetes00561.txt',\n",
              " 'Diabetes00562.txt',\n",
              " 'Diabetes00563.txt',\n",
              " 'Diabetes00564.txt',\n",
              " 'Diabetes00565.txt',\n",
              " 'Diabetes00566.txt',\n",
              " 'Diabetes00567.txt',\n",
              " 'Diabetes00568.txt',\n",
              " 'Diabetes00569.txt',\n",
              " 'Diabetes00570.txt',\n",
              " 'Diabetes00571.txt',\n",
              " 'Diabetes00572.txt',\n",
              " 'Diabetes00573.txt',\n",
              " 'Diabetes00574.txt',\n",
              " 'Diabetes00575.txt',\n",
              " 'Diabetes00576.txt',\n",
              " 'Diabetes00577.txt',\n",
              " 'Diabetes00578.txt',\n",
              " 'Diabetes00579.txt',\n",
              " 'Diabetes00580.txt',\n",
              " 'Diabetes00581.txt',\n",
              " 'Diabetes00582.txt',\n",
              " 'Diabetes00583.txt',\n",
              " 'Diabetes00584.txt',\n",
              " 'Diabetes00585.txt',\n",
              " 'Diabetes00586.txt',\n",
              " 'Diabetes00587.txt',\n",
              " 'Diabetes00588.txt',\n",
              " 'Diabetes00589.txt',\n",
              " 'Diabetes00590.txt',\n",
              " 'Diabetes00591.txt',\n",
              " 'Diabetes00592.txt',\n",
              " 'Diabetes00593.txt',\n",
              " 'Diabetes00594.txt',\n",
              " 'Diabetes00595.txt',\n",
              " 'Diabetes00596.txt',\n",
              " 'Diabetes00597.txt',\n",
              " 'Diabetes00598.txt',\n",
              " 'Diabetes00599.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEITAV0z11ns"
      },
      "source": [
        "#create a list of news articles\n",
        "news = []\n",
        "for file in filelists.fileids():\n",
        "    news.append(filelists.raw(file))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "wkMn62EY11ns",
        "outputId": "3c0c230a-7eea-45ef-932f-c16d253f6bf6"
      },
      "source": [
        "news[25]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'New type of cell movement discovered\\n <p>For decades, researchers have used petri dishes to study cell movement. These classic tissue culture tools, however, only permit two-dimensional movement, very different from the three-dimensional movements that cells make in a human body.</p><div class=\"photobox_right\" style=\\'max-width:349px;\\' ><img src=\"http://www.medicalnewstoday.com/images/articles/281/281729/cell-moving-through-a-3-d-matrix.jpg\" alt=\"Cell Moving through a 3-D Matrix\"><br><em>Penn and NIH researchers measured the internal pressure of individual fibroblast cells (in orange) moving through a three-dimensional matrix (in blue). They found that, in this environment, the cells\\' nuclei operate like an engine\\'s piston to push the cell forward.<br>Credit: University of Pennsylvania/NIDCR</em></div><p>In a new study from the University of Pennsylvania and National Institute of Dental and Craniofacial Research, scientists used an innovative technique to study how cells move in a three-dimensional matrix, similar to the structure of certain tissues, such as the skin. They discovered an entirely new type of cell movement whereby the nucleus helps propel cells through the matrix like a piston in an engine, generating pressure that thrusts the cell\\'s plasma membrane forward.</p><p>\"Our work elucidated a highly intriguing question: how cells move when they are in the complex and physiologically relevant environment of a 3-D extracellular matrix,\" said Hyun (Michel) Koo, a professor in the Department of Orthodontics at Penn\\'s School of Dental Medicine. \"We discovered that the nucleus can act as a piston that physically compartmentalizes the cell cytoplasm and increases the hydrostatic pressure driving the cell motility within a 3-D matrix.\"</p><p>Koo worked with lead author Ryan Petrie and senior author Kenneth Yamada, both of the National Institutes of Health\\'s NIDCR, on the study, which is published in<em> Science.</em></p><p>\"We think it\\'s a very important normal physiological mechanism of cell movement that has not been characterized previously,\" Petrie said.</p><p>The team studied fibroblasts, the most common type of cell found in connective tissue. Fibroblasts themselves produce proteins including collagen and fibronectin that connect in a complex matrix that is found in the skin, the intestines and other tissues in the body.</p><p>The researchers used this fibroblast-created matrix to test how cells migrate through a three-dimensional structure. The matrix is crosslinked, meaning its fibers are resistant to bending and flexing as the cells move through.</p><p>Studies of fibroblasts on two-dimensional surfaces indicated that the most typical form of movement involved protrusions called lamellipodia, created by the polymerization of the protein actin into fibers that push the cell membrane forward. In 2012, however, Petrie and Yamada showed that when fibroblasts migrate they can switch to a different movement strategy when placed in a three-dimensional matrix, using blunt protrusions called lobopodia.</p><p>What the researchers did not know was how these lobopodia were formed. Suspecting they might be generated from increased intracellular pressure, the team used sophisticated microelectrodes to measure the hydrostatic pressure of the fluid inside the cell. They found that the pressure was significantly higher in cells moving in a three-dimensional extracellular matrix compared to cells moving along a two-dimensional surface or in a three-dimensional matrix that wasn\\'t cross linked like the fibroblast-derived matrix.</p><p>To drill down further and see how the pressure was distributed within the cell moving in a three-dimensional matrix, they measured pressure from in front of and behind the nucleus. Cells moving using lobopodia have elevated pressure in front of the nucleus but not behind it, generating energy to propel the cell forward. Using live-cell confocal microscopy, they observed that the nucleus could be pulled forward, away from the rear of the cell, with the nucleus dividing the cell into low-pressure and high-pressure compartments.</p><p>\"When a cell is in the matrix, the nucleus tends to be at the back of the cell, and the cell body is very tubular in shape,\" Petrie said. \"It really looked like a piston.\"</p><p>They found that the nucleus is actually pulled forward by the actin filaments that connect the nucleus to the front of the cell. This movement \"pressurizes\" the cell. The scientists were also able to identify the protein components responsible for moving the nuclear piston, including actomyosin, vimentin and nesprin.</p><p>\"The pressure itself is what pushes the plasma membrane,\" Petrie said.</p><p>Because this only happened to cells moving in the three-dimensional cell-created matrix and not cells moving in other substrates, the researchers note that the cells must be sensing their physical environment to determine what type of movement to use.</p><p>This type of cell migration might be common in other tissues of the body, the researchers noted. When they took chondrocytes from knee cartilage and myofibroblasts from intestinal tissue and placed them in the matrix, those cells used the same type of lobopodia-driven motion.</p><p>The discovery could have implications for understanding diseases such as cancer as well, because cancerous cells tend to move in distinct ways from normal cells.</p><p>\"It might give us leverage to find out what is unique about cancer cells so we can target them therapeutically and not affect normal cells,\" Petrie said.</p><p>Importantly, these findings could have broader relevance to other biological systems where living cells are enmeshed within and surrounded by an extracellular matrix, such as in biofilms, which are associated with many human infectious diseases.</p><p>\"This work illustrates how the physical structure of the matrix can influence cellular properties to govern biological function,\" Koo said. \"We are now applying these fascinating principles to further understand how biofilm matrix modulates bacterial virulence to cause oral diseases, such as dental caries.\"</p> \\nFor decades, researchers have used petri dishes to study cell movement. These classic tissue culture tools, however, only permit two-dimensional movement, very different from the...\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngB7kF8g11nt"
      },
      "source": [
        "# import the shorttext library for text preprocessing\n",
        "#standard_text_preprocessor_1 under shorttext.utils provides a standard way of text preprocessing, including the following steps:\n",
        "\n",
        "   #1. removing special characters,\n",
        "   #2. removing numerals,\n",
        "   #3. converting all alphabets to lower cases,\n",
        "   #4. removing stop words, and\n",
        "   #5. stemming the words (using Porter stemmer).\n",
        "\n",
        "from shorttext.utils import standard_text_preprocessor_1, DocumentTermMatrix\n",
        "preprocessor = standard_text_preprocessor_1()\n",
        "corpus = [preprocessor(article).split(' ') for article in news]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wz8YLx1GYqB",
        "outputId": "eab9936d-2882-4b53-98a1-f25d858f65b1"
      },
      "source": [
        "corpus[25]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['new',\n",
              " 'type',\n",
              " 'cell',\n",
              " 'movement',\n",
              " 'discovered\\n',\n",
              " 'pfor',\n",
              " 'decad',\n",
              " 'research',\n",
              " 'use',\n",
              " 'petri',\n",
              " 'dish',\n",
              " 'studi',\n",
              " 'cell',\n",
              " 'movement',\n",
              " 'classic',\n",
              " 'tissu',\n",
              " 'cultur',\n",
              " 'tool',\n",
              " 'howev',\n",
              " 'permit',\n",
              " 'twodimension',\n",
              " 'movement',\n",
              " 'differ',\n",
              " 'threedimension',\n",
              " 'movement',\n",
              " 'cell',\n",
              " 'make',\n",
              " 'human',\n",
              " 'bodypdiv',\n",
              " 'classphotobox_right',\n",
              " 'stylemaxwidthpx',\n",
              " 'img',\n",
              " 'srchttpwwwmedicalnewstodaycomimagesarticlescellmovingthroughadmatrixjpg',\n",
              " 'altcel',\n",
              " 'move',\n",
              " 'matrixbrempenn',\n",
              " 'nih',\n",
              " 'research',\n",
              " 'measur',\n",
              " 'intern',\n",
              " 'pressur',\n",
              " 'individu',\n",
              " 'fibroblast',\n",
              " 'cell',\n",
              " 'orang',\n",
              " 'move',\n",
              " 'threedimension',\n",
              " 'matrix',\n",
              " 'blue',\n",
              " 'found',\n",
              " 'environ',\n",
              " 'cell',\n",
              " 'nuclei',\n",
              " 'oper',\n",
              " 'like',\n",
              " 'engin',\n",
              " 'piston',\n",
              " 'push',\n",
              " 'cell',\n",
              " 'forwardbrcredit',\n",
              " 'universiti',\n",
              " 'pennsylvanianidcremdivpin',\n",
              " 'new',\n",
              " 'studi',\n",
              " 'universiti',\n",
              " 'pennsylvania',\n",
              " 'nation',\n",
              " 'institut',\n",
              " 'dental',\n",
              " 'craniofaci',\n",
              " 'research',\n",
              " 'scientist',\n",
              " 'use',\n",
              " 'innov',\n",
              " 'techniqu',\n",
              " 'studi',\n",
              " 'cell',\n",
              " 'move',\n",
              " 'threedimension',\n",
              " 'matrix',\n",
              " 'similar',\n",
              " 'structur',\n",
              " 'certain',\n",
              " 'tissu',\n",
              " 'skin',\n",
              " 'discov',\n",
              " 'entir',\n",
              " 'new',\n",
              " 'type',\n",
              " 'cell',\n",
              " 'movement',\n",
              " 'wherebi',\n",
              " 'nucleus',\n",
              " 'help',\n",
              " 'propel',\n",
              " 'cell',\n",
              " 'matrix',\n",
              " 'like',\n",
              " 'piston',\n",
              " 'engin',\n",
              " 'generat',\n",
              " 'pressur',\n",
              " 'thrust',\n",
              " 'cell',\n",
              " 'plasma',\n",
              " 'membran',\n",
              " 'forwardppour',\n",
              " 'work',\n",
              " 'elucid',\n",
              " 'high',\n",
              " 'intrigu',\n",
              " 'question',\n",
              " 'cell',\n",
              " 'move',\n",
              " 'complex',\n",
              " 'physiolog',\n",
              " 'relev',\n",
              " 'environ',\n",
              " 'extracellular',\n",
              " 'matrix',\n",
              " 'said',\n",
              " 'hyun',\n",
              " 'michel',\n",
              " 'koo',\n",
              " 'professor',\n",
              " 'depart',\n",
              " 'orthodont',\n",
              " 'penn',\n",
              " 'school',\n",
              " 'dental',\n",
              " 'medicin',\n",
              " 'discov',\n",
              " 'nucleus',\n",
              " 'act',\n",
              " 'piston',\n",
              " 'physic',\n",
              " 'compartment',\n",
              " 'cell',\n",
              " 'cytoplasm',\n",
              " 'increas',\n",
              " 'hydrostat',\n",
              " 'pressur',\n",
              " 'drive',\n",
              " 'cell',\n",
              " 'motil',\n",
              " 'within',\n",
              " 'matrixppkoo',\n",
              " 'work',\n",
              " 'lead',\n",
              " 'author',\n",
              " 'ryan',\n",
              " 'petri',\n",
              " 'senior',\n",
              " 'author',\n",
              " 'kenneth',\n",
              " 'yamada',\n",
              " 'nation',\n",
              " 'institut',\n",
              " 'health',\n",
              " 'nidcr',\n",
              " 'studi',\n",
              " 'publish',\n",
              " 'inem',\n",
              " 'scienceemppw',\n",
              " 'think',\n",
              " 'import',\n",
              " 'normal',\n",
              " 'physiolog',\n",
              " 'mechan',\n",
              " 'cell',\n",
              " 'movement',\n",
              " 'character',\n",
              " 'previous',\n",
              " 'petri',\n",
              " 'saidppth',\n",
              " 'team',\n",
              " 'studi',\n",
              " 'fibroblast',\n",
              " 'common',\n",
              " 'type',\n",
              " 'cell',\n",
              " 'found',\n",
              " 'connect',\n",
              " 'tissu',\n",
              " 'fibroblast',\n",
              " 'produc',\n",
              " 'protein',\n",
              " 'includ',\n",
              " 'collagen',\n",
              " 'fibronectin',\n",
              " 'connect',\n",
              " 'complex',\n",
              " 'matrix',\n",
              " 'found',\n",
              " 'skin',\n",
              " 'intestin',\n",
              " 'tissu',\n",
              " 'bodyppth',\n",
              " 'research',\n",
              " 'use',\n",
              " 'fibroblastcr',\n",
              " 'matrix',\n",
              " 'test',\n",
              " 'cell',\n",
              " 'migrat',\n",
              " 'threedimension',\n",
              " 'structur',\n",
              " 'matrix',\n",
              " 'crosslink',\n",
              " 'mean',\n",
              " 'fiber',\n",
              " 'resist',\n",
              " 'bend',\n",
              " 'flex',\n",
              " 'cell',\n",
              " 'move',\n",
              " 'throughppstudi',\n",
              " 'fibroblast',\n",
              " 'twodimension',\n",
              " 'surfac',\n",
              " 'indic',\n",
              " 'typic',\n",
              " 'form',\n",
              " 'movement',\n",
              " 'involv',\n",
              " 'protrus',\n",
              " 'call',\n",
              " 'lamellipodia',\n",
              " 'creat',\n",
              " 'polymer',\n",
              " 'protein',\n",
              " 'actin',\n",
              " 'fiber',\n",
              " 'push',\n",
              " 'cell',\n",
              " 'membran',\n",
              " 'forward',\n",
              " '',\n",
              " 'howev',\n",
              " 'petri',\n",
              " 'yamada',\n",
              " 'show',\n",
              " 'fibroblast',\n",
              " 'migrat',\n",
              " 'switch',\n",
              " 'differ',\n",
              " 'movement',\n",
              " 'strategi',\n",
              " 'place',\n",
              " 'threedimension',\n",
              " 'matrix',\n",
              " 'use',\n",
              " 'blunt',\n",
              " 'protrus',\n",
              " 'call',\n",
              " 'lobopodiappwhat',\n",
              " 'research',\n",
              " 'know',\n",
              " 'lobopodia',\n",
              " 'form',\n",
              " 'suspect',\n",
              " 'might',\n",
              " 'generat',\n",
              " 'increas',\n",
              " 'intracellular',\n",
              " 'pressur',\n",
              " 'team',\n",
              " 'use',\n",
              " 'sophist',\n",
              " 'microelectrod',\n",
              " 'measur',\n",
              " 'hydrostat',\n",
              " 'pressur',\n",
              " 'fluid',\n",
              " 'insid',\n",
              " 'cell',\n",
              " 'found',\n",
              " 'pressur',\n",
              " 'signific',\n",
              " 'higher',\n",
              " 'cell',\n",
              " 'move',\n",
              " 'threedimension',\n",
              " 'extracellular',\n",
              " 'matrix',\n",
              " 'compar',\n",
              " 'cell',\n",
              " 'move',\n",
              " 'along',\n",
              " 'twodimension',\n",
              " 'surfac',\n",
              " 'threedimension',\n",
              " 'matrix',\n",
              " 'wasnt',\n",
              " 'cross',\n",
              " 'link',\n",
              " 'like',\n",
              " 'fibroblastderiv',\n",
              " 'matrixppto',\n",
              " 'drill',\n",
              " 'see',\n",
              " 'pressur',\n",
              " 'distribut',\n",
              " 'within',\n",
              " 'cell',\n",
              " 'move',\n",
              " 'threedimension',\n",
              " 'matrix',\n",
              " 'measur',\n",
              " 'pressur',\n",
              " 'front',\n",
              " 'behind',\n",
              " 'nucleus',\n",
              " 'cell',\n",
              " 'move',\n",
              " 'use',\n",
              " 'lobopodia',\n",
              " 'elev',\n",
              " 'pressur',\n",
              " 'front',\n",
              " 'nucleus',\n",
              " 'behind',\n",
              " 'generat',\n",
              " 'energi',\n",
              " 'propel',\n",
              " 'cell',\n",
              " 'forward',\n",
              " 'use',\n",
              " 'livecel',\n",
              " 'confoc',\n",
              " 'microscopi',\n",
              " 'observ',\n",
              " 'nucleus',\n",
              " 'could',\n",
              " 'pull',\n",
              " 'forward',\n",
              " 'away',\n",
              " 'rear',\n",
              " 'cell',\n",
              " 'nucleus',\n",
              " 'divid',\n",
              " 'cell',\n",
              " 'lowpressur',\n",
              " 'highpressur',\n",
              " 'compartmentsppwhen',\n",
              " 'cell',\n",
              " 'matrix',\n",
              " 'nucleus',\n",
              " 'tend',\n",
              " 'back',\n",
              " 'cell',\n",
              " 'cell',\n",
              " 'bodi',\n",
              " 'tubular',\n",
              " 'shape',\n",
              " 'petri',\n",
              " 'said',\n",
              " 'realli',\n",
              " 'look',\n",
              " 'like',\n",
              " 'pistonppthey',\n",
              " 'found',\n",
              " 'nucleus',\n",
              " 'actual',\n",
              " 'pull',\n",
              " 'forward',\n",
              " 'actin',\n",
              " 'filament',\n",
              " 'connect',\n",
              " 'nucleus',\n",
              " 'front',\n",
              " 'cell',\n",
              " 'movement',\n",
              " 'pressur',\n",
              " 'cell',\n",
              " 'scientist',\n",
              " 'abl',\n",
              " 'identifi',\n",
              " 'protein',\n",
              " 'compon',\n",
              " 'respons',\n",
              " 'move',\n",
              " 'nuclear',\n",
              " 'piston',\n",
              " 'includ',\n",
              " 'actomyosin',\n",
              " 'vimentin',\n",
              " 'nesprinppth',\n",
              " 'pressur',\n",
              " 'push',\n",
              " 'plasma',\n",
              " 'membran',\n",
              " 'petri',\n",
              " 'saidppbecaus',\n",
              " 'happen',\n",
              " 'cell',\n",
              " 'move',\n",
              " 'threedimension',\n",
              " 'cellcreat',\n",
              " 'matrix',\n",
              " 'cell',\n",
              " 'move',\n",
              " 'substrat',\n",
              " 'research',\n",
              " 'note',\n",
              " 'cell',\n",
              " 'must',\n",
              " 'sens',\n",
              " 'physic',\n",
              " 'environ',\n",
              " 'determin',\n",
              " 'type',\n",
              " 'movement',\n",
              " 'useppthi',\n",
              " 'type',\n",
              " 'cell',\n",
              " 'migrat',\n",
              " 'might',\n",
              " 'common',\n",
              " 'tissu',\n",
              " 'bodi',\n",
              " 'research',\n",
              " 'note',\n",
              " 'took',\n",
              " 'chondrocyt',\n",
              " 'knee',\n",
              " 'cartilag',\n",
              " 'myofibroblast',\n",
              " 'intestin',\n",
              " 'tissu',\n",
              " 'place',\n",
              " 'matrix',\n",
              " 'cell',\n",
              " 'use',\n",
              " 'type',\n",
              " 'lobopodiadriven',\n",
              " 'motionppth',\n",
              " 'discoveri',\n",
              " 'could',\n",
              " 'implic',\n",
              " 'understand',\n",
              " 'diseas',\n",
              " 'cancer',\n",
              " 'well',\n",
              " 'cancer',\n",
              " 'cell',\n",
              " 'tend',\n",
              " 'move',\n",
              " 'distinct',\n",
              " 'way',\n",
              " 'normal',\n",
              " 'cellsppit',\n",
              " 'might',\n",
              " 'give',\n",
              " 'us',\n",
              " 'leverag',\n",
              " 'find',\n",
              " 'uniqu',\n",
              " 'cancer',\n",
              " 'cell',\n",
              " 'target',\n",
              " 'therapeut',\n",
              " 'affect',\n",
              " 'normal',\n",
              " 'cell',\n",
              " 'petri',\n",
              " 'saidppimport',\n",
              " 'find',\n",
              " 'could',\n",
              " 'broader',\n",
              " 'relev',\n",
              " 'biolog',\n",
              " 'system',\n",
              " 'live',\n",
              " 'cell',\n",
              " 'enmesh',\n",
              " 'within',\n",
              " 'surround',\n",
              " 'extracellular',\n",
              " 'matrix',\n",
              " 'biofilm',\n",
              " 'associ',\n",
              " 'mani',\n",
              " 'human',\n",
              " 'infecti',\n",
              " 'diseasesppthi',\n",
              " 'work',\n",
              " 'illustr',\n",
              " 'physic',\n",
              " 'structur',\n",
              " 'matrix',\n",
              " 'influenc',\n",
              " 'cellular',\n",
              " 'properti',\n",
              " 'govern',\n",
              " 'biolog',\n",
              " 'function',\n",
              " 'koo',\n",
              " 'said',\n",
              " 'appli',\n",
              " 'fascin',\n",
              " 'principl',\n",
              " 'understand',\n",
              " 'biofilm',\n",
              " 'matrix',\n",
              " 'modul',\n",
              " 'bacteri',\n",
              " 'virul',\n",
              " 'caus',\n",
              " 'oral',\n",
              " 'diseas',\n",
              " 'dental',\n",
              " 'cariesp',\n",
              " '\\nfor',\n",
              " 'decad',\n",
              " 'research',\n",
              " 'use',\n",
              " 'petri',\n",
              " 'dish',\n",
              " 'studi',\n",
              " 'cell',\n",
              " 'movement',\n",
              " 'classic',\n",
              " 'tissu',\n",
              " 'cultur',\n",
              " 'tool',\n",
              " 'howev',\n",
              " 'permit',\n",
              " 'twodimension',\n",
              " 'movement',\n",
              " 'differ',\n",
              " 'the\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q3gIccD11nu"
      },
      "source": [
        "#convert the corpus of news to a document term matrix (dtm), where each row represents a document, each column represents the number of occurence for a word\n",
        "\n",
        "dtm = DocumentTermMatrix(corpus, docids = filelists.fileids())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EdyfZ7Y11nw",
        "outputId": "83f94733-2905-4c0c-e513-bb4ce3ea3ae4"
      },
      "source": [
        "#check number of occurence of the word \"associ\" in each document\n",
        "dtm.get_token_occurences('associ')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Dentistry00500.txt': 5.0,\n",
              " 'Dentistry00502.txt': 2.0,\n",
              " 'Dentistry00503.txt': 1.0,\n",
              " 'Dentistry00505.txt': 1.0,\n",
              " 'Dentistry00506.txt': 1.0,\n",
              " 'Dentistry00512.txt': 2.0,\n",
              " 'Dentistry00514.txt': 8.0,\n",
              " 'Dentistry00515.txt': 4.0,\n",
              " 'Dentistry00516.txt': 1.0,\n",
              " 'Dentistry00518.txt': 2.0,\n",
              " 'Dentistry00521.txt': 1.0,\n",
              " 'Dentistry00523.txt': 7.0,\n",
              " 'Dentistry00524.txt': 1.0,\n",
              " 'Dentistry00525.txt': 2.0,\n",
              " 'Dentistry00526.txt': 1.0,\n",
              " 'Dentistry00528.txt': 3.0,\n",
              " 'Dentistry00529.txt': 1.0,\n",
              " 'Dentistry00530.txt': 8.0,\n",
              " 'Dentistry00533.txt': 2.0,\n",
              " 'Dentistry00534.txt': 1.0,\n",
              " 'Dentistry00535.txt': 1.0,\n",
              " 'Dentistry00536.txt': 2.0,\n",
              " 'Dentistry00537.txt': 3.0,\n",
              " 'Dentistry00538.txt': 2.0,\n",
              " 'Dentistry00541.txt': 3.0,\n",
              " 'Dentistry00542.txt': 1.0,\n",
              " 'Dentistry00544.txt': 1.0,\n",
              " 'Dentistry00548.txt': 4.0,\n",
              " 'Dentistry00549.txt': 2.0,\n",
              " 'Dentistry00550.txt': 2.0,\n",
              " 'Dentistry00552.txt': 3.0,\n",
              " 'Dentistry00553.txt': 2.0,\n",
              " 'Dentistry00554.txt': 1.0,\n",
              " 'Dentistry00555.txt': 1.0,\n",
              " 'Dentistry00556.txt': 3.0,\n",
              " 'Dentistry00557.txt': 2.0,\n",
              " 'Dentistry00558.txt': 2.0,\n",
              " 'Dentistry00559.txt': 1.0,\n",
              " 'Depression00500.txt': 1.0,\n",
              " 'Depression00501.txt': 1.0,\n",
              " 'Depression00505.txt': 2.0,\n",
              " 'Depression00506.txt': 2.0,\n",
              " 'Depression00509.txt': 1.0,\n",
              " 'Depression00514.txt': 1.0,\n",
              " 'Depression00515.txt': 1.0,\n",
              " 'Depression00518.txt': 3.0,\n",
              " 'Depression00520.txt': 2.0,\n",
              " 'Depression00525.txt': 2.0,\n",
              " 'Depression00527.txt': 6.0,\n",
              " 'Depression00528.txt': 7.0,\n",
              " 'Depression00529.txt': 1.0,\n",
              " 'Depression00531.txt': 2.0,\n",
              " 'Depression00532.txt': 2.0,\n",
              " 'Depression00533.txt': 5.0,\n",
              " 'Depression00534.txt': 1.0,\n",
              " 'Depression00535.txt': 3.0,\n",
              " 'Depression00536.txt': 2.0,\n",
              " 'Depression00537.txt': 2.0,\n",
              " 'Depression00538.txt': 2.0,\n",
              " 'Depression00539.txt': 4.0,\n",
              " 'Diabetes00500.txt': 1.0,\n",
              " 'Diabetes00502.txt': 1.0,\n",
              " 'Diabetes00503.txt': 1.0,\n",
              " 'Diabetes00504.txt': 3.0,\n",
              " 'Diabetes00506.txt': 1.0,\n",
              " 'Diabetes00508.txt': 1.0,\n",
              " 'Diabetes00509.txt': 2.0,\n",
              " 'Diabetes00510.txt': 4.0,\n",
              " 'Diabetes00513.txt': 4.0,\n",
              " 'Diabetes00515.txt': 4.0,\n",
              " 'Diabetes00518.txt': 1.0,\n",
              " 'Diabetes00520.txt': 1.0,\n",
              " 'Diabetes00522.txt': 1.0,\n",
              " 'Diabetes00523.txt': 2.0,\n",
              " 'Diabetes00528.txt': 3.0,\n",
              " 'Diabetes00530.txt': 1.0,\n",
              " 'Diabetes00533.txt': 1.0,\n",
              " 'Diabetes00534.txt': 2.0,\n",
              " 'Diabetes00535.txt': 9.0,\n",
              " 'Diabetes00537.txt': 1.0,\n",
              " 'Diabetes00538.txt': 1.0,\n",
              " 'Diabetes00540.txt': 1.0,\n",
              " 'Diabetes00541.txt': 4.0,\n",
              " 'Diabetes00543.txt': 1.0,\n",
              " 'Diabetes00544.txt': 1.0,\n",
              " 'Diabetes00545.txt': 4.0,\n",
              " 'Diabetes00547.txt': 2.0,\n",
              " 'Diabetes00549.txt': 2.0,\n",
              " 'Diabetes00550.txt': 2.0,\n",
              " 'Diabetes00551.txt': 1.0,\n",
              " 'Diabetes00553.txt': 1.0,\n",
              " 'Diabetes00554.txt': 3.0,\n",
              " 'Diabetes00557.txt': 2.0,\n",
              " 'Diabetes00558.txt': 2.0,\n",
              " 'Diabetes00559.txt': 3.0,\n",
              " 'Diabetes00560.txt': 2.0,\n",
              " 'Diabetes00563.txt': 2.0,\n",
              " 'Diabetes00565.txt': 1.0,\n",
              " 'Diabetes00567.txt': 1.0,\n",
              " 'Diabetes00570.txt': 1.0,\n",
              " 'Diabetes00571.txt': 3.0,\n",
              " 'Diabetes00573.txt': 2.0,\n",
              " 'Diabetes00575.txt': 2.0,\n",
              " 'Diabetes00576.txt': 1.0,\n",
              " 'Diabetes00577.txt': 1.0,\n",
              " 'Diabetes00578.txt': 5.0,\n",
              " 'Diabetes00579.txt': 1.0,\n",
              " 'Diabetes00580.txt': 2.0,\n",
              " 'Diabetes00581.txt': 1.0,\n",
              " 'Diabetes00582.txt': 8.0,\n",
              " 'Diabetes00583.txt': 3.0,\n",
              " 'Diabetes00584.txt': 2.0,\n",
              " 'Diabetes00585.txt': 1.0,\n",
              " 'Diabetes00587.txt': 1.0,\n",
              " 'Diabetes00589.txt': 2.0,\n",
              " 'Diabetes00591.txt': 3.0,\n",
              " 'Diabetes00592.txt': 2.0,\n",
              " 'Diabetes00593.txt': 1.0,\n",
              " 'Diabetes00594.txt': 2.0,\n",
              " 'Diabetes00595.txt': 3.0,\n",
              " 'Diabetes00596.txt': 1.0,\n",
              " 'Diabetes00597.txt': 2.0}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIxeUJC311nx",
        "outputId": "cc1235bd-a285-4e85-ef5d-005ca658d6b8"
      },
      "source": [
        "type(dtm.get_token_occurences('associ'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbJ8iXsx11nx"
      },
      "source": [
        "# define a function to calculate entropy given a distribution\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def entropy(p):\n",
        "    if sum(p) == 0:\n",
        "        return 0\n",
        "\n",
        "    p = p/sum(p)\n",
        "\n",
        "    H = -sum(p*np.log2(p))\n",
        "\n",
        "    return H"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95tk0Fav11ny",
        "outputId": "aea70a3d-4529-4727-861e-9bf57e7f4832"
      },
      "source": [
        "entropy(np.array( [12,45]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7424875695421236"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxSIDSQB11ny",
        "outputId": "525304e6-b1a7-4270-916a-a4e55ff4edae"
      },
      "source": [
        "entropy(np.array( [100,60,40]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4854752972273344"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c1Icrx062Wl",
        "outputId": "d3d11c7c-99fa-4ee8-ff27-f35247f67da3"
      },
      "source": [
        "#convert the documents that contain the word \"associ\" from dictionary keys to a list\n",
        "associ_docs = list(dtm.get_token_occurences('associ').keys())\n",
        "associ_docs"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dentistry00500.txt',\n",
              " 'Dentistry00502.txt',\n",
              " 'Dentistry00503.txt',\n",
              " 'Dentistry00505.txt',\n",
              " 'Dentistry00506.txt',\n",
              " 'Dentistry00512.txt',\n",
              " 'Dentistry00514.txt',\n",
              " 'Dentistry00515.txt',\n",
              " 'Dentistry00516.txt',\n",
              " 'Dentistry00518.txt',\n",
              " 'Dentistry00521.txt',\n",
              " 'Dentistry00523.txt',\n",
              " 'Dentistry00524.txt',\n",
              " 'Dentistry00525.txt',\n",
              " 'Dentistry00526.txt',\n",
              " 'Dentistry00528.txt',\n",
              " 'Dentistry00529.txt',\n",
              " 'Dentistry00530.txt',\n",
              " 'Dentistry00533.txt',\n",
              " 'Dentistry00534.txt',\n",
              " 'Dentistry00535.txt',\n",
              " 'Dentistry00536.txt',\n",
              " 'Dentistry00537.txt',\n",
              " 'Dentistry00538.txt',\n",
              " 'Dentistry00541.txt',\n",
              " 'Dentistry00542.txt',\n",
              " 'Dentistry00544.txt',\n",
              " 'Dentistry00548.txt',\n",
              " 'Dentistry00549.txt',\n",
              " 'Dentistry00550.txt',\n",
              " 'Dentistry00552.txt',\n",
              " 'Dentistry00553.txt',\n",
              " 'Dentistry00554.txt',\n",
              " 'Dentistry00555.txt',\n",
              " 'Dentistry00556.txt',\n",
              " 'Dentistry00557.txt',\n",
              " 'Dentistry00558.txt',\n",
              " 'Dentistry00559.txt',\n",
              " 'Depression00500.txt',\n",
              " 'Depression00501.txt',\n",
              " 'Depression00505.txt',\n",
              " 'Depression00506.txt',\n",
              " 'Depression00509.txt',\n",
              " 'Depression00514.txt',\n",
              " 'Depression00515.txt',\n",
              " 'Depression00518.txt',\n",
              " 'Depression00520.txt',\n",
              " 'Depression00525.txt',\n",
              " 'Depression00527.txt',\n",
              " 'Depression00528.txt',\n",
              " 'Depression00529.txt',\n",
              " 'Depression00531.txt',\n",
              " 'Depression00532.txt',\n",
              " 'Depression00533.txt',\n",
              " 'Depression00534.txt',\n",
              " 'Depression00535.txt',\n",
              " 'Depression00536.txt',\n",
              " 'Depression00537.txt',\n",
              " 'Depression00538.txt',\n",
              " 'Depression00539.txt',\n",
              " 'Diabetes00500.txt',\n",
              " 'Diabetes00502.txt',\n",
              " 'Diabetes00503.txt',\n",
              " 'Diabetes00504.txt',\n",
              " 'Diabetes00506.txt',\n",
              " 'Diabetes00508.txt',\n",
              " 'Diabetes00509.txt',\n",
              " 'Diabetes00510.txt',\n",
              " 'Diabetes00513.txt',\n",
              " 'Diabetes00515.txt',\n",
              " 'Diabetes00518.txt',\n",
              " 'Diabetes00520.txt',\n",
              " 'Diabetes00522.txt',\n",
              " 'Diabetes00523.txt',\n",
              " 'Diabetes00528.txt',\n",
              " 'Diabetes00530.txt',\n",
              " 'Diabetes00533.txt',\n",
              " 'Diabetes00534.txt',\n",
              " 'Diabetes00535.txt',\n",
              " 'Diabetes00537.txt',\n",
              " 'Diabetes00538.txt',\n",
              " 'Diabetes00540.txt',\n",
              " 'Diabetes00541.txt',\n",
              " 'Diabetes00543.txt',\n",
              " 'Diabetes00544.txt',\n",
              " 'Diabetes00545.txt',\n",
              " 'Diabetes00547.txt',\n",
              " 'Diabetes00549.txt',\n",
              " 'Diabetes00550.txt',\n",
              " 'Diabetes00551.txt',\n",
              " 'Diabetes00553.txt',\n",
              " 'Diabetes00554.txt',\n",
              " 'Diabetes00557.txt',\n",
              " 'Diabetes00558.txt',\n",
              " 'Diabetes00559.txt',\n",
              " 'Diabetes00560.txt',\n",
              " 'Diabetes00563.txt',\n",
              " 'Diabetes00565.txt',\n",
              " 'Diabetes00567.txt',\n",
              " 'Diabetes00570.txt',\n",
              " 'Diabetes00571.txt',\n",
              " 'Diabetes00573.txt',\n",
              " 'Diabetes00575.txt',\n",
              " 'Diabetes00576.txt',\n",
              " 'Diabetes00577.txt',\n",
              " 'Diabetes00578.txt',\n",
              " 'Diabetes00579.txt',\n",
              " 'Diabetes00580.txt',\n",
              " 'Diabetes00581.txt',\n",
              " 'Diabetes00582.txt',\n",
              " 'Diabetes00583.txt',\n",
              " 'Diabetes00584.txt',\n",
              " 'Diabetes00585.txt',\n",
              " 'Diabetes00587.txt',\n",
              " 'Diabetes00589.txt',\n",
              " 'Diabetes00591.txt',\n",
              " 'Diabetes00592.txt',\n",
              " 'Diabetes00593.txt',\n",
              " 'Diabetes00594.txt',\n",
              " 'Diabetes00595.txt',\n",
              " 'Diabetes00596.txt',\n",
              " 'Diabetes00597.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeR5RJE111n3",
        "outputId": "d73e0979-3d9f-40b7-8bc2-b145450ce41e"
      },
      "source": [
        "#print the number of \"Dentistry\" documents that contain the word \"associ\"\n",
        "sub = 'Dentistry'\n",
        "count_dentistry = 0\n",
        "for item in associ_docs:\n",
        "    if sub in item:\n",
        "        count_dentistry += 1\n",
        "print(count_dentistry)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wdk9UEi11n3",
        "outputId": "feeb6366-7c10-4ba7-928b-784ecd2f1644"
      },
      "source": [
        "#print the number of \"Depression\" documents that contain the word \"associ\"\n",
        "sub = 'Depression'\n",
        "count_depression = 0\n",
        "for item in associ_docs:\n",
        "    if sub in item:\n",
        "        count_depression += 1\n",
        "print(count_depression)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj_DaAt-11n3",
        "outputId": "f37fd675-b807-493a-a6ef-5dbf97269e34"
      },
      "source": [
        "#print the number of \"Diabetes\" documents that contain the word \"associ\"\n",
        "sub = 'Diabetes'\n",
        "count_diabetes = 0\n",
        "for item in associ_docs:\n",
        "    if sub in item:\n",
        "        count_diabetes += 1\n",
        "print(count_diabetes)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaYaXFq_11n4"
      },
      "source": [
        "#this is an important step\n",
        "#make an array, rows represent \"Dentistry\", \"Depression\", and \"Diabetes\" respectively. Columns represent the number of documents that contains the word \"associ\" and the number of documents that do NOT contain the word \"associ\"\n",
        "array = np.reshape((count_dentistry,60-count_dentistry,count_depression,40-count_depression,count_diabetes,100-count_diabetes ),(3,2))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-SCYZln11n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8021a94b-6d51-48dc-ef8b-f99a823d8f60"
      },
      "source": [
        "array"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[38, 22],\n",
              "       [22, 18],\n",
              "       [62, 38]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OZfgAh911n4",
        "outputId": "25278203-3585-4f66-fd32-0e5485fc5d44"
      },
      "source": [
        "#calculate the column sum of the array\n",
        "np.sum(array, axis = 0)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([122,  78])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW4ReSBb84Ax",
        "outputId": "f9321842-1678-49e5-a88c-2b10162967b9"
      },
      "source": [
        "#calculate the entroy of the word \"associ\"\n",
        "entropy(np.array( [122,78]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9647995485050872"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkYMxRge11n5"
      },
      "source": [
        "#calculate the entropy of the document class (Dentistry, Depression, and Diabetes)\n",
        "entropy_class = entropy(np.array([60,40,100]))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4Q7JtfhIMp4",
        "outputId": "dfac4a5c-d17f-4d22-f669-8a674a8064ac"
      },
      "source": [
        "entropy_class"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4854752972273344"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAJcI5ia11n5"
      },
      "source": [
        "#calculate the column probabilities\n",
        "column_probs = np.sum(array, axis = 0)/200"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKkw-a_N11n5",
        "outputId": "fcc895d0-8509-4518-b572-c4a3adb3114a"
      },
      "source": [
        "column_probs"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.61, 0.39])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTDc2Tzs11n6"
      },
      "source": [
        "#calculate the column entropies\n",
        "column_entropy = np.apply_along_axis(entropy, 0, array)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56pEgC_k11n6",
        "outputId": "0a35db76-d33e-468b-cf3c-25d7a85b4629"
      },
      "source": [
        "column_entropy"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.46607412, 1.50864079])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdfwdl4pI5Ft",
        "outputId": "f72d9286-b3a4-46c4-e3d2-3c57927e2a51"
      },
      "source": [
        "#Two random variables: X (document class) and Y (associ's presense or absence). I (X;Y) = H(X) - H(X|Y)\n",
        "column_probs*column_entropy"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.89430521, 0.58836991])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VLtYoEO11n7"
      },
      "source": [
        "conditional_entropy = sum(column_probs*column_entropy)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZpDh86R11n7",
        "outputId": "e0ab4133-da53-4901-be22-8647db2a825e"
      },
      "source": [
        "conditional_entropy"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4826751237111302"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNtZDVHa11n8"
      },
      "source": [
        "#calculate the mutual information between the word \"associ\" and the document class (Dentistry, Depression, and Diabetes)\n",
        "# the approach above used entropy reduction for the mutual information instead of joint probability\n",
        "MI_associ = entropy_class - conditional_entropy"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQZZ2dnS11n8",
        "outputId": "d273a72f-363f-4915-f620-b12fb2949fa9"
      },
      "source": [
        "MI_associ"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0028001735162042074"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OWO3PIP11n8"
      },
      "source": [],
      "execution_count": 38,
      "outputs": []
    }
  ]
}